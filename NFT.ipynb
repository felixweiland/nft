{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NFT.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPq5bXfZ1/JrheGiMrxyp8G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/felixweiland/nft/blob/main/NFT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5EZjdD_tkmD"
      },
      "outputs": [],
      "source": [
        "# Mount Goole Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "% cd gdrive/MyDrive/Colab\\ Notebooks"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image \n",
        "from IPython.display import display \n",
        "import random\n",
        "import json\n",
        "import pandas as pd\n",
        "import os\n",
        "from os import walk\n",
        "from tqdm import tqdm, tqdm_notebook"
      ],
      "metadata": {
        "id": "-TLtmRyftoYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining the user\n",
        "total_images = int(input(\"How many would you like to create? \"))\n",
        "user = input(\"Who is the user? \")\n",
        "project_name = input(\"Name of project: \")\n",
        "\n",
        "#Just making sure that all the correct folders exists\n",
        "if project_name not in os.listdir(f'/content/gdrive/MyDrive/NFT/{user}/'):\n",
        "  print(\"Make sure that you have created a directory with the same name that contains the layers\")\n",
        "\n",
        "for folder in [\"images\", \"meta_data\"]:\n",
        "  if folder not in os.listdir(f'/content/gdrive/MyDrive/NFT/{user}/test/'):\n",
        "    os.mkdir(f'/content/gdrive/MyDrive/NFT/{user}/test/{folder}')\n",
        "\n",
        "if \"image_data\" not in os.listdir(f'/content/gdrive/MyDrive/NFT/{user}/test/meta_data/'):\n",
        "  os.mkdir(f'/content/gdrive/MyDrive/NFT/{user}/test/meta_data/image_data')\n",
        "\n",
        "#Defining a path variable to make the code adaptable \n",
        "path = f\"/content/gdrive/MyDrive/NFT/{user}/{project_name}/\""
      ],
      "metadata": {
        "id": "Eupc2mojubSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining all variables\n",
        "\n",
        "#First of all I am checking whether or not there is a file already, if this script has been run before in the same project.\n",
        "#If it has I want to see if the user wants to change any info, or keep the same. If No I import the old data, if Yes we input all again. \n",
        "if \"project_info.json\" in os.listdir(f\"/content/gdrive/MyDrive/NFT/{user}/{project_name}/meta_data/\"):\n",
        "  if input(\"Do you want to change the current info of the project? (Y/N) \") == \"Y\":\n",
        "      with open(f\"{path}/meta_data/project_info.json\") as outfile:\n",
        "        base_info = json.load(outfile)\n",
        "\n",
        "        #Printing the current values and asking for new input\n",
        "        print(\"Current base_uri: \", base_info[\"base_uri\"])\n",
        "        base_uri = input(\"Full url to base-uri: \")\n",
        "\n",
        "        print(f\"Current description: \", base_info[\"description\"])\n",
        "        description = input(\"Description of the project: \")\n",
        "\n",
        "        print(f\"Current backgrounds: \", base_info[\"backgrounds\"])\n",
        "        x = True\n",
        "        while x:\n",
        "          truth = []\n",
        "          backgrounds = input(\"HEX color codes for background, seperated by commas and without the #: \").split(\",\")\n",
        "          for i, item in enumerate(backgrounds):\n",
        "            backgrounds[i] = item.strip()\n",
        "            truth.append(len(backgrounds[i])==6)\n",
        "          if False not in truth:\n",
        "            x = False\n",
        "          else:\n",
        "            print(\"Please make sure that the input is correct.\")\n",
        "\n",
        "        weights = {}\n",
        "\n",
        "      #Generating a dictionary for export\n",
        "      base_info = {\"base_uri\": base_uri,\n",
        "                    \"project_name\": project_name,\n",
        "                    \"description\": description,\n",
        "                    \"backgrounds\": backgrounds \n",
        "      }\n",
        "\n",
        "      with open(f\"{path}/meta_data/project_info.json\", \"w\") as start_file:\n",
        "        json.dump(base_info, start_file, indent=4)\n",
        "\n",
        "  else:\n",
        "    with open(f\"{path}/meta_data/project_info.json\") as start_file:\n",
        "      project_data = json.load(start_file)\n",
        "    \n",
        "    base_uri = project_data[\"base_uri\"]\n",
        "    project_name = project_data[\"project_name\"]\n",
        "    description = project_data[\"description\"]\n",
        "    backgrounds = project_data[\"backgrounds\"]\n",
        "    weights = {}\n",
        "\n",
        "    print(\"Successfully loaded all data: \")\n",
        "    for item in project_data.items():\n",
        "      print(item)\n",
        "\n",
        "#If there is no project, get the input from the user and then save the file. \n",
        "else:\n",
        "    base_uri = input(\"Full url to base-uri: \")\n",
        "    description = input(\"Description of the project: \")\n",
        "\n",
        "    #To make sure that the formatting of the HEX is correct\n",
        "    x = True\n",
        "    while x:\n",
        "      truth = []\n",
        "      backgrounds = input(\"HEX color codes for background, seperated by commas and without the #: \").split(\",\")\n",
        "      for i, item in enumerate(backgrounds):\n",
        "        backgrounds[i] = item.strip()\n",
        "        truth.append(len(backgrounds[i])==6)\n",
        "      if False not in truth:\n",
        "        x = False\n",
        "\n",
        "    weights = {}\n",
        "\n",
        "    base_info = {\"base_uri\": base_uri,\n",
        "                  \"project_name\": project_name,\n",
        "                  \"description\": description,\n",
        "                  \"backgrounds\": backgrounds \n",
        "    }\n",
        "\n",
        "    with open(f\"{path}/meta_data/project_info.json\", \"w\") as start_file:\n",
        "      json.dump(base_info, start_file, indent=4)"
      ],
      "metadata": {
        "id": "jCc_pNBlzM_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Here I want to see if the script has been run before and that there are previously generated images. I don't want any doubles, so if there is\n",
        "#I will import that file as all_images. If not all_images is an empty list. \n",
        "try: \n",
        "  with open(f\"{path}/meta_data/all-traits.json\") as outfile:\n",
        "    all_images = json.load(outfile)\n",
        "    #images_to_generate is a list that will contain all new pictures that are to be generated. \n",
        "    images_to_generate = []\n",
        "    #Here I am taking in a number to be used later in order to filter out already inputed items.\n",
        "    delete_past = len(all_images)\n",
        "\n",
        "except:\n",
        "  all_images = []\n",
        "  images_to_generate = []\n",
        "  delete_past = 0"
      ],
      "metadata": {
        "id": "V44qbXrVy0tP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dirpaths = []\n",
        "layers = []\n",
        "clean_layers = []\n",
        "path_dictionary = {}\n",
        "\n",
        "#A function that creates all the baseline stuff. I get all different folders available and all the files that they contain.\n",
        "#I append them to the above stated lists and dictionaries. \n",
        "def gen_paths(path):\n",
        "    \n",
        "    \"\"\"\n",
        "    Generates all the folders nessecary for the program. Just insert the folder path to the layers as a string.\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    #Creating a list with all the paths and making sure that they are in the correct order. \n",
        "    for (dirpath, dirnames, filenames) in walk(f'{path}layers/'):\n",
        "        if dirpath not in dirpaths:\n",
        "            dirpaths.append(dirpath)\n",
        "    dirpaths.sort()\n",
        "        \n",
        "    #Appending all filenames to each respective folder in the path\n",
        "    for (dirpath, dirnames, filenames) in walk(f'{path}layers/'):\n",
        "\n",
        "        dirpath = dirpath.split(\"/\")\n",
        "\n",
        "        if len(dirpath[-1]) > 2:\n",
        "          path_dictionary[dirpath[-1]] = []\n",
        "            \n",
        "    #Creating a list with all the layers\n",
        "    for item in path_dictionary.keys():\n",
        "        if item not in layers:\n",
        "            layers.append(item)\n",
        "    layers.sort()\n",
        "\n",
        "    #And one that is without the order\n",
        "    for item in layers:\n",
        "        if item not in clean_layers:\n",
        "            clean_layers.append(item.split(\". \")[1])\n",
        "    \n",
        "    #Appending all file names to the respective folder\n",
        "    for item in path_dictionary.keys():\n",
        "    \n",
        "        for (dirpath, dirnames, filenames) in walk(f\"{path}/layers/{item}\"):\n",
        "            path_dictionary[item].extend(filenames)\n",
        "            break\n",
        "            \n",
        "gen_paths(path)"
      ],
      "metadata": {
        "id": "kOBclCFRup0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a dictionary for different weights.\n",
        "def gen_weights():\n",
        "    \n",
        "    \"\"\"\n",
        "    Used to calculate different weights for attributes. Iterates over each layer and takes user input. Input - to create equal weights.\n",
        "    \"\"\"\n",
        "    print(\"Input weights for the attributes and seperate by comma.\\nThey need to add up to 100 and have the same number of items as printed.\\nInput - if you want to have the same probabiltiy for all\")\n",
        "    \n",
        "    #Iterating over each key in the path_dictionary.\n",
        "    for key in path_dictionary.keys():\n",
        "\n",
        "        #A placeholder list\n",
        "        item_list = []\n",
        "        error = True\n",
        "\n",
        "        #Taking input from the user regarding the weights\n",
        "        while error:\n",
        "          w_input = input(f\"{len(path_dictionary[key])} items. \\nWeights for {path_dictionary[key]}: \")\n",
        "\n",
        "          if len(w_input.split(\",\")) == len(path_dictionary[key]) or w_input == \"-\":\n",
        "            error = False\n",
        "          else:\n",
        "            print(\"Not valid input.. Try again.\")\n",
        "\n",
        "        #Checking to see if the user wants to have the same weights.\n",
        "        if w_input != \"-\":\n",
        "            for item in w_input.split(\",\"):\n",
        "                item_list.append(int(item))\n",
        "\n",
        "        #Calculating the weights and appending them to the list\n",
        "        else:\n",
        "            item_list.append(100/len(path_dictionary[key]))\n",
        "            item_list = item_list*len(path_dictionary[key])\n",
        "\n",
        "        #Assingning the list to each respective key.\n",
        "        weights[key] = item_list\n",
        "        \n",
        "    #Exporting the file\n",
        "    data_file = f'{path}/meta_data/weights.json';\n",
        "    with open(data_file, 'w') as outfile:\n",
        "      json.dump(weights, outfile, indent=4)"
      ],
      "metadata": {
        "id": "9ktT_wGVuuPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  with open(f\"{path}/meta_data/weights.json\") as outfile:\n",
        "    weights = json.load(outfile)\n",
        "    for item in weights.items():\n",
        "      print(item)\n",
        "\n",
        "  if input(\"Do you want to change the weights from the previous settings? (Y/N)\") == \"Y\":\n",
        "    gen_weights()\n",
        "  \n",
        "  else:\n",
        "    print(\"Proceeding with the current weights\")\n",
        "\n",
        "except:\n",
        "  gen_weights()"
      ],
      "metadata": {
        "id": "XsiTTQIUZXis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_new_image():\n",
        "    \n",
        "    #Creating a placeholder dictionary\n",
        "    new_image = {} \n",
        "    new_image[f\"{1 + len(path_dictionary.keys())}. tokenId\"] = delete_past+i\n",
        "    new_image[f\"{2 + len(path_dictionary.keys())}. background\"] = random.choices(backgrounds)[0]\n",
        "\n",
        "    for key in path_dictionary.keys():\n",
        "\n",
        "        #Creating keys without the positioning numbers\n",
        "        new_image[key] = random.choices(path_dictionary[key], weights[key])[0]\n",
        "        \n",
        "    #Return the dictionary if not already in all_images. This to prevet doubles. \n",
        "    if new_image not in all_images:\n",
        "        return new_image \n",
        "    \n",
        "    #If the image already exists - run the function again.\n",
        "    else:\n",
        "        return create_new_image()\n",
        "\n",
        "# Generate the unique combinations based on trait weightings and appending an unique ID\n",
        "for i in range(total_images):\n",
        "\n",
        "    new_trait_image = create_new_image()\n",
        "    all_images.append(new_trait_image)\n",
        "    images_to_generate.append(new_trait_image)\n",
        "\n",
        "    #Generating a random background.\n",
        "    background = random.choices(backgrounds)[0]"
      ],
      "metadata": {
        "id": "lyCSjfsruzJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Just to make sure that the images that are to be generated are the same as in the all_images file. \n",
        "def sanity_check():\n",
        "  for i in range(len(images_to_generate)):\n",
        "    truth = []\n",
        "    truth.append(images_to_generate[i] == all_images[delete_past+i])\n",
        "  if False not in truth:\n",
        "    print(\"All checks out\")\n",
        "\n",
        "  else:\n",
        "    raise SanityError(\"There seems to be some differences, please look over the content again\")\n",
        "\n",
        "sanity_check()"
      ],
      "metadata": {
        "id": "iH8Dh1Vtj15o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cleaning up the attributes for export\n",
        "meta_data = []\n",
        "\n",
        "for item in all_images:\n",
        "    \n",
        "    data = {}\n",
        "    \n",
        "    for key, value in item.items():\n",
        "\n",
        "      #To remove the file-type from the data files\n",
        "      if str(value)[-4:] == \".png\" or str(value)[-4:] == \".jpg\":\n",
        "          try:\n",
        "            data[key.split(\". \")[1]] = value.split(\".\")[0]\n",
        "          except:\n",
        "            data[key] = value.split(\".\")[0]\n",
        "\n",
        "      #For all values that does not have a file appendix\n",
        "      else:\n",
        "          try:\n",
        "            data[key.split(\". \")[1]] = value\n",
        "          except:\n",
        "            data[key] = value\n",
        "        \n",
        "    meta_data.append(data)\n",
        "\n",
        "#Creating a json-file with the traits.\n",
        "meta_data_file = f'{path}/meta_data/all-traits.json'; \n",
        "with open(meta_data_file, 'w') as outfile:\n",
        "  json.dump(meta_data, outfile, indent=4)\n",
        "\n",
        "#Creating an excel sheet with all information - for the user to get an overview.\n",
        "excel_export = pd.DataFrame(meta_data)\n",
        "excel_export.to_excel(f'{path}/meta_data/all-traits.xlsx')"
      ],
      "metadata": {
        "id": "D06IQ6OEu4Nq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_image():\n",
        "    \n",
        "    #Iterating over all items in all_images dictionary\n",
        "    for item in tqdm(images_to_generate):\n",
        "        \n",
        "        #Creating an empty dictonary to hold all the image-files for each layer of the composition. Resets for every new one.\n",
        "        image = {}\n",
        "        \n",
        "        #Iterating over the keys together with an index locator appending them to the path-dictionary with the index as a key\n",
        "        #This will help in the later execution of the code.\n",
        "        try:\n",
        "          for i, key in enumerate(path_dictionary.keys()):  \n",
        "              image[i] = Image.open(f'{dirpaths[i+1]}/{item[layers[i]]}').convert('RGBA')\n",
        "\n",
        "        except:\n",
        "          for i, key in enumerate(layers):  \n",
        "              image[i] = Image.open(f'{dirpaths[i+1]}/{item[clean_layers[i]]}').convert('RGBA')\n",
        "\n",
        "        #Creating a baseline image with the first and second element of the picture\n",
        "        composite_image = Image.alpha_composite(image[0], image[1])\n",
        "\n",
        "        #Iterating over the keys and values in the image-dictionary to extract the specific image-layer\n",
        "        for key, value in image.items():\n",
        "            \n",
        "            #Since we used the first two index counts, we restrict this loop while the key is less than the\n",
        "            #lenght of image. This helps the script from breaking while allowing any number of layers.\n",
        "            if key < len(image)-2:\n",
        "                \n",
        "                #Continously adding the different layers to the composition\n",
        "                composite_image = Image.alpha_composite(composite_image, image[key+2])\n",
        "                \n",
        "        #Saving the composition as the tokenId in png-format\n",
        "        final_image = composite_image.convert('RGB')\n",
        "        try:\n",
        "          file_name = str(item[f'{1+len(path_dictionary.keys())}. tokenId']) + \".png\"\n",
        "        except:  \n",
        "          file_name = str(item['tokenId']) + \".png\"\n",
        "        final_image.save(f\"{path}/images/\" + file_name)\n",
        "        \n",
        "gen_image()"
      ],
      "metadata": {
        "id": "IZ8GIoKr1ic2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function that takes in a key and a value and returns them inside a dictionary with keys \"trait-type\" and \"value\"\n",
        "def getAttribute(key, value):\n",
        "    \"\"\"\n",
        "    Function that takes in a key and a value and returns them inside a dictionary with keys \"trait-type\" and \"value\"\n",
        "    \"\"\"\n",
        "    return {\"trait_type\": key, \"value\": value}\n",
        "\n",
        "#Function that generates the meta-data per image and saves it as a json-file\n",
        "def yield_attributes(data_file):\n",
        "    \"\"\"\n",
        "    Yields NFT-attributes from json-file. Enter path to file.\n",
        "    \"\"\"\n",
        "    file = open(data_file,) \n",
        "    data = json.load(file)\n",
        "\n",
        "    #Iterating over the dictonaries in the data file while excluding previously generated files.\n",
        "    for idx, item in enumerate(data[delete_past:]):\n",
        "        token_id = item['tokenId']\n",
        "\n",
        "        #Because there are two ways that the background is named I added this error handling. \n",
        "        try:\n",
        "          token = {\n",
        "              \"image\": base_uri + str(token_id) + '.png',\n",
        "              \"tokenId\": token_id,\n",
        "              \"name\": project_name + ' #' + str(token_id),\n",
        "              \"description\":description,\n",
        "              \"background_color\":all_images[idx][f\"{2 + len(path_dictionary.keys())}. background\"],\n",
        "              \"attributes\": []\n",
        "          }\n",
        "\n",
        "        except:\n",
        "          token = {\n",
        "            \"image\": base_uri + str(token_id) + '.png',\n",
        "            \"tokenId\": token_id,\n",
        "            \"name\": project_name + ' #' + str(token_id),\n",
        "            \"description\":description,\n",
        "            \"background_color\":all_images[idx][\"background\"],\n",
        "            \"attributes\": []\n",
        "          }\n",
        "\n",
        "        for i, _ in enumerate(clean_layers):\n",
        "            token[\"attributes\"].append(getAttribute(clean_layers[i], item[clean_layers[i]]))\n",
        "\n",
        "        with open(f'{path}/meta_data/image_data/' + str(token_id) +\".json\", 'w') as outfile:\n",
        "            json.dump(token, outfile, indent=4)\n",
        "    file.close()\n",
        "    \n",
        "yield_attributes(f'{path}/meta_data/all-traits.json')"
      ],
      "metadata": {
        "id": "kbVLCwGtvEEl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}